{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mxfXul2RxlRB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy.__version__:   1.11.1\n",
      "tensorflow_privacy.__version__:   0.8.10\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# tensorflow imports\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# tensorflow-privacy\n",
    "import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from numpy.random import choice, seed\n",
    "from numpy import ndarray, concatenate, stack, array, round, zeros, arange\n",
    "import numpy as np\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pickle\n",
    "\n",
    "from os import mkdir, path\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from warnings import simplefilter\n",
    "simplefilter('ignore', category=FutureWarning)\n",
    "simplefilter('ignore', category=DeprecationWarning)\n",
    "\n",
    "import scipy\n",
    "print(\"scipy.__version__:  \", scipy.__version__)\n",
    "\n",
    "import tensorflow_privacy.privacy\n",
    "print(\"tensorflow_privacy.__version__:  \", tensorflow_privacy.__version__)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the input raw datasets \n",
    "#### 100 set of 1000 records each (= IN dataset for MIA RF-> get pred from classifier)\n",
    "#### RAW data which which seeded the DP data. This will be needed for the MIA attack. \n",
    "#### But traning of the classifier will be done using the DP data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"/home/privacy-master/DataShare/Adult_Raw_100set_1k.pkl\",\"rb\")\n",
    "HundredSet=pickle.load(filehandler)\n",
    "filehandler.close()\n",
    "for i in range(len(HundredSet)):\n",
    "    HundredSet[i][\"income\"] = np.where(HundredSet[i]['income'] == '<=50K' , 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read the TEST (OUT) set (= test set for the classifier; = test set for MIA RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "0    3761\n",
       "1    1239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filehandler = open(f'/home/privacy-master/DataShare/Adult_test5k_capped.pkl',\"rb\")\n",
    "dfTest=pickle.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "dfTest [\"income\"] = np.where(dfTest['income'] == '<=50K' , 0, 1)\n",
    "dfTest[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read the synthetic sets (= train dataset for the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= [\"500\"]# ts for tabDDPM\n",
    "\n",
    "san_k = epochs\n",
    "dataFiles={}\n",
    "for ts in epochs:\n",
    "    filehandler = open(f'/home/privacy-master/DataShare/Adult_ddpm_500.pkl',\"rb\")\n",
    "    dataFiles[ts]=pickle.load(filehandler)\n",
    "    filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = dfTest['income']\n",
    "X_test = dfTest.drop(['income'], axis=1)\n",
    "\n",
    "categorical = [col for col in X_test.columns if X_test[col].dtypes == 'O']\n",
    "numerical = [col for col in X_test.columns if X_test[col].dtypes != 'O']\n",
    "\n",
    "encoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country'])\n",
    "\n",
    "encoder.fit(dfTest.drop([\"income\"], axis=1))\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score={}\n",
    "MIA_score={}\n",
    "for epoc in epochs:\n",
    "    utility_score[epoc]={}\n",
    "    MIA_score[epoc]={}\n",
    "    for set_no in range(len(HundredSet)):\n",
    "        utility_score[epoc][set_no]=[]\n",
    "        MIA_score[epoc][set_no]=[]\n",
    "        \n",
    "        dataFile = dataFiles[epoc][set_no]\n",
    "        for serialID in range(2):#10\n",
    "\n",
    "            df=dataFile[serialID]\n",
    "            \n",
    "            if (len(df)==0):\n",
    "                continue\n",
    "            count = df['income'].value_counts()\n",
    "            #print(count)\n",
    "            \n",
    "            INset=HundredSet[set_no]\n",
    "\n",
    "            y_train = df['income']\n",
    "            X_train = df.drop(['income'], axis=1)\n",
    "            #print(X_train.shape, X_test.shape)\n",
    "            X_train = encoder.transform(X_train)\n",
    "            \n",
    "            \n",
    "            INset_y_train = INset['income']\n",
    "            INset_X_train = INset.drop(['income'], axis=1)\n",
    "            INset_X_train = encoder.transform(INset_X_train)\n",
    "                        \n",
    "\n",
    "            cols = X_train.columns\n",
    "            scaler = RobustScaler()\n",
    "            X_test = scaler.fit_transform(X_test)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            INset_X_train = scaler.transform(INset_X_train)\n",
    "\n",
    "            X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "            X_test = pd.DataFrame(X_test, columns=[cols])\n",
    "            INset_X_train = pd.DataFrame(INset_X_train, columns=[cols])\n",
    "            \n",
    "\n",
    "            # instantiate the classifier with n_estimators = 100\n",
    "            # fit the model to the training set\n",
    "            #rfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "            rfc_100=xgb.XGBClassifier()\n",
    "            \n",
    "            le = LabelEncoder().fit(y_train)\n",
    "            y_train = le.transform(y_train)\n",
    "            \n",
    "            rfc_100.fit(X_train, y_train)\n",
    "            # Predict on the test set results\n",
    "            y_pred_100_enc = rfc_100.predict(X_test)\n",
    "            y_pred_100 = le.inverse_transform(y_pred_100_enc)\n",
    "            \n",
    "            # Check accuracy score \n",
    "            acc = accuracy_score(y_test, y_pred_100)\n",
    "            #print('Model accuracy score with epsilon {} DP: {:0.4f}'.format(epoc,acc))\n",
    "            utility_score[epoc][set_no].append(acc)\n",
    "            \n",
    "\n",
    "            ## victim classifier has been trained with sanitized data.\n",
    "        \n",
    "            INx_train_pred1 = rfc_100.predict_proba(INset_X_train)\n",
    "            OUTx_test_pred1 = rfc_100.predict_proba(X_test)\n",
    "            \n",
    "            \n",
    "            \n",
    "            INx_train_pred = np.zeros((len(INset_X_train), 2))\n",
    "            INx_train_pred[0:len(INset_X_train), 0:INx_train_pred1.shape[1]]=INx_train_pred1\n",
    "            OUTx_test_pred = np.zeros((len(X_test), 2))\n",
    "            OUTx_test_pred[0:len(X_test), 0:OUTx_test_pred1.shape[1]] = OUTx_test_pred1\n",
    "            \n",
    "            \n",
    "            probs_train = INx_train_pred\n",
    "            probs_test = OUTx_test_pred\n",
    "            #labels_train = y_train\n",
    "            labels_train = np.zeros((len(INx_train_pred)))\n",
    "            labels_train[0:len(INx_train_pred)] = INset_y_train\n",
    "            labels_train = labels_train.astype(int)\n",
    "            #labels_test = y_test\n",
    "            labels_test = np.zeros((len(OUTx_test_pred)))\n",
    "            labels_test[0:len(OUTx_test_pred)] = y_test\n",
    "            labels_test=labels_test.astype(int)\n",
    "\n",
    "            # define what variables our attacker should have access to\n",
    "            attack_input = AttackInputData(\n",
    "              probs_train = probs_train,\n",
    "              probs_test = probs_test,\n",
    "              #loss_train = loss_train,\n",
    "              #loss_test = loss_test,\n",
    "              labels_train = labels_train,\n",
    "              labels_test = labels_test,\n",
    "              multilabel_data = False#True\n",
    "            )\n",
    "\n",
    "            \n",
    "            slicing_spec = SlicingSpec(\n",
    "                entire_dataset = True,\n",
    "                by_class = False,\n",
    "                by_percentiles = False,\n",
    "                by_classification_correctness = False)\n",
    "\n",
    "            # define the type of attacker model that we want to use\n",
    "            attack_types = [\n",
    "                #AttackType.THRESHOLD_ATTACK,\n",
    "                AttackType.RANDOM_FOREST#LOGISTIC_REGRESSION\n",
    "            ]\n",
    "\n",
    "            # run the attack\n",
    "            attacks_result = mia.run_attacks(attack_input=attack_input,\n",
    "                                             slicing_spec=slicing_spec,\n",
    "                                             balance_attacker_training = False,\n",
    "                                             attack_types=attack_types)\n",
    "\n",
    "            #print(attacks_result.summary(by_slices=True))                        \n",
    "            MIA_score[epoc][set_no].append(attacks_result.summary(by_slices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: \n",
      "eps = 500\n",
      "0.77\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.79\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.79\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.74\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.79\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.79\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.73\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.78\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.65\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.76\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "0.79\n",
      "2\n",
      "0.77\n",
      "2\n",
      "0.75\n",
      "2\n",
      "Average: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Accuracy: \")\n",
    "for ts in epochs:\n",
    "    print(f\"eps = {ts}\")\n",
    "    all_id_sum=0\n",
    "    for set_no in range(len(HundredSet)):\n",
    "        all_id_sum+= ((sum(utility_score[ts][set_no]))/len(utility_score[ts][set_no]))\n",
    "        print(\"%.2f\"% ((sum(utility_score[ts][set_no]))/len(utility_score[ts][set_no])))\n",
    "        print(len(utility_score[ts][set_no]))\n",
    "    print(\"Average: %.2f\"% (all_id_sum/len(HundredSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIA result: \n",
      "eps = 500\n",
      "AUC: 0.72 \tADV: 0.47 \tPPV: 0.62\n",
      "AUC: 0.96 \tADV: 0.84 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.94\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.98\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.97\n",
      "AUC: 0.97 \tADV: 0.89 \tPPV: 0.98\n",
      "AUC: 0.93 \tADV: 0.78 \tPPV: 0.96\n",
      "AUC: 0.97 \tADV: 0.92 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.82 \tPPV: 0.95\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.98\n",
      "AUC: 0.93 \tADV: 0.80 \tPPV: 0.94\n",
      "AUC: 0.95 \tADV: 0.81 \tPPV: 0.97\n",
      "AUC: 0.97 \tADV: 0.92 \tPPV: 0.98\n",
      "AUC: 0.96 \tADV: 0.85 \tPPV: 0.97\n",
      "AUC: 0.96 \tADV: 0.88 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.95\n",
      "AUC: 0.96 \tADV: 0.88 \tPPV: 0.99\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.96\n",
      "AUC: 0.95 \tADV: 0.86 \tPPV: 0.97\n",
      "AUC: 0.92 \tADV: 0.76 \tPPV: 0.94\n",
      "AUC: 0.93 \tADV: 0.78 \tPPV: 0.91\n",
      "AUC: 0.92 \tADV: 0.73 \tPPV: 0.95\n",
      "AUC: 0.98 \tADV: 0.94 \tPPV: 0.99\n",
      "AUC: 0.96 \tADV: 0.84 \tPPV: 0.97\n",
      "AUC: 0.95 \tADV: 0.81 \tPPV: 0.96\n",
      "AUC: 0.95 \tADV: 0.81 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.86 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.99\n",
      "AUC: 0.95 \tADV: 0.85 \tPPV: 0.95\n",
      "AUC: 0.96 \tADV: 0.88 \tPPV: 0.98\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.81 \tPPV: 0.95\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.97\n",
      "AUC: 0.94 \tADV: 0.79 \tPPV: 0.93\n",
      "AUC: 0.96 \tADV: 0.87 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.80 \tPPV: 0.97\n",
      "AUC: 0.93 \tADV: 0.76 \tPPV: 0.97\n",
      "AUC: 0.94 \tADV: 0.81 \tPPV: 0.97\n",
      "AUC: 0.96 \tADV: 0.89 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.80 \tPPV: 0.97\n",
      "AUC: 0.92 \tADV: 0.75 \tPPV: 0.91\n",
      "AUC: 0.92 \tADV: 0.76 \tPPV: 0.95\n",
      "AUC: 0.93 \tADV: 0.79 \tPPV: 0.95\n",
      "AUC: 0.95 \tADV: 0.81 \tPPV: 0.98\n",
      "AUC: 0.93 \tADV: 0.79 \tPPV: 0.92\n",
      "AUC: 0.96 \tADV: 0.90 \tPPV: 0.97\n",
      "AUC: 0.92 \tADV: 0.75 \tPPV: 0.93\n",
      "AUC: 0.96 \tADV: 0.84 \tPPV: 0.98\n",
      "AUC: 0.93 \tADV: 0.79 \tPPV: 0.96\n",
      "AUC: 0.93 \tADV: 0.79 \tPPV: 0.95\n",
      "AUC: 0.94 \tADV: 0.83 \tPPV: 0.95\n",
      "AUC: 0.95 \tADV: 0.84 \tPPV: 0.96\n",
      "AUC: 0.97 \tADV: 0.89 \tPPV: 0.98\n",
      "AUC: 0.96 \tADV: 0.89 \tPPV: 0.97\n",
      "AUC: 0.97 \tADV: 0.92 \tPPV: 0.99\n",
      "AUC: 0.96 \tADV: 0.87 \tPPV: 0.98\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.80 \tPPV: 0.96\n",
      "AUC: 0.94 \tADV: 0.78 \tPPV: 0.97\n",
      "AUC: 0.95 \tADV: 0.82 \tPPV: 0.96\n",
      "AUC: 0.96 \tADV: 0.87 \tPPV: 0.97\n",
      "AUC: 0.97 \tADV: 0.90 \tPPV: 0.98\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.82 \tPPV: 0.97\n",
      "AUC: 0.94 \tADV: 0.83 \tPPV: 0.97\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.97\n",
      "AUC: 0.95 \tADV: 0.82 \tPPV: 0.94\n",
      "AUC: 0.94 \tADV: 0.81 \tPPV: 0.97\n",
      "AUC: 0.94 \tADV: 0.83 \tPPV: 0.96\n",
      "AUC: 0.92 \tADV: 0.72 \tPPV: 0.91\n",
      "AUC: 0.97 \tADV: 0.92 \tPPV: 0.99\n",
      "AUC: 0.94 \tADV: 0.84 \tPPV: 0.98\n",
      "AUC: 0.97 \tADV: 0.92 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.99\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.97\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.97\n",
      "AUC: 0.96 \tADV: 0.87 \tPPV: 0.98\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.96\n",
      "AUC: 0.97 \tADV: 0.89 \tPPV: 0.97\n",
      "AUC: 0.96 \tADV: 0.86 \tPPV: 0.97\n",
      "AUC: 0.95 \tADV: 0.82 \tPPV: 0.97\n",
      "AUC: 0.93 \tADV: 0.79 \tPPV: 0.95\n",
      "AUC: 0.95 \tADV: 0.84 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.84 \tPPV: 0.97\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 1.00\n",
      "AUC: 0.97 \tADV: 0.89 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.82 \tPPV: 0.97\n",
      "AUC: 0.96 \tADV: 0.88 \tPPV: 0.97\n",
      "AUC: 0.93 \tADV: 0.77 \tPPV: 0.95\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.94\n",
      "AUC: 0.97 \tADV: 0.89 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.83 \tPPV: 0.98\n",
      "AUC: 0.95 \tADV: 0.83 \tPPV: 0.96\n",
      "AUC: 0.94 \tADV: 0.81 \tPPV: 0.97\n",
      "AUC: 0.94 \tADV: 0.81 \tPPV: 0.95\n",
      "AUC: 0.94 \tADV: 0.83 \tPPV: 0.94\n",
      "AUC: 0.95 \tADV: 0.84 \tPPV: 0.98\n",
      "AUC: 0.94 \tADV: 0.84 \tPPV: 0.95\n",
      "AUC: 0.95 \tADV: 0.86 \tPPV: 0.96\n",
      "AUC: 0.96 \tADV: 0.89 \tPPV: 0.96\n",
      "Average AUC: 0.95 \tADV: 0.83 \tPPV: 0.96\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(\"MIA result: \")\n",
    "for ts in epochs:\n",
    "    print(f\"eps = {ts}\")\n",
    "    all_id_AUCsum, all_id_ADVsum, all_id_PPVsum = 0, 0, 0\n",
    "    \n",
    "    for set_no in range(len(HundredSet)):#\n",
    "        temp_AUCsum, temp_ADVsum, temp_PPVsum = 0, 0, 0\n",
    "        for str1 in MIA_score[ts][set_no]:       \n",
    "            result = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", str1)\n",
    "            temp_AUCsum += float(result[2])\n",
    "            temp_ADVsum += float(result[5])\n",
    "            temp_PPVsum += float(result[8])\n",
    "        temp_AUCsum /= len(MIA_score[ts][set_no])#100\n",
    "        temp_ADVsum /= len(MIA_score[ts][set_no])#100\n",
    "        temp_PPVsum /= len(MIA_score[ts][set_no])#100\n",
    "        print(\"AUC: %.2f \\tADV: %.2f \\tPPV: %.2f\"% (temp_AUCsum, temp_ADVsum, temp_PPVsum))\n",
    "        all_id_AUCsum += temp_AUCsum\n",
    "        all_id_ADVsum += temp_ADVsum\n",
    "        all_id_PPVsum += temp_PPVsum\n",
    "    print(\"Average AUC: %.2f \\tADV: %.2f \\tPPV: %.2f\"% (all_id_AUCsum/len(HundredSet), all_id_ADVsum/len(HundredSet), all_id_PPVsum/len(HundredSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tfMIA",
   "language": "python",
   "name": "tfmia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
